{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMnTLNrwixd8"
      },
      "source": [
        "# Ejercicio de programación Regresión Lineal Multiple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 476,
      "metadata": {
        "id": "GGF64D8mixd-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Computacion vectorial y cientifica para python\n",
        "import numpy as np\n",
        "\n",
        "# Librerias para graficación (trazado de gráficos)\n",
        "from matplotlib import pyplot\n",
        "from mpl_toolkits.mplot3d import Axes3D  # Necesario para graficar superficies 3D\n",
        "from google.colab import drive\n",
        "# llama a matplotlib a embeber graficas dentro de los cuadernillos\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOqT_Rmj2Ekl",
        "outputId": "ba9d5ed4-44cb-404e-8ee7-47479d7fbc75"
      },
      "execution_count": 477,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEoJidm7ixeA"
      },
      "source": [
        "## 2 Regresión lineal con multiples variables\n",
        "\n",
        "Se implementa la regresion lineal multivariable para predecir el precio de las casas. El archivo `Datasets/ex1data2.txt` contiene un conjunto de entrenamiento de precios de casas en Portland, Oregon. La primera columna es el tamaño de la casa en metros cuadrados, la segunda columna es el numero de cuartos, y la tercera columna es el precio de la casa. \n",
        "\n",
        "<a id=\"section4\"></a>\n",
        "### 2.1 Normalización de caracteristicas\n",
        "\n",
        "Al visualizar los datos se puede observar que las caracteristicas tienen diferentes magnitudes, por lo cual se debe transformar cada valor en una escala de valores similares, esto con el fin de que el descenso por el gradiente pueda converger mas rapidamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 478,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NtHZuZoixeA",
        "outputId": "3d005685-c712-4f6e-e9ab-11e8c7faa25c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.81 3.79 3.28 10.22 4.22 6.5 2.93 4.7 0.28 1.93 4.13 7.2 3.6 2.53 0.24\n",
            " 0.97 0.41 3.54 4.16 6.04 4.18 3.84 0.06 0.47 5.38 5.65 5.32 1.87 0.13\n",
            " 3.12 0.11 4.34 0.35 0.65 0.07 0.08 0.49 0.3 2.66 0.48 5.33 2.67 0.13 0.36\n",
            " 3.96 1.91 1.1 1.2 3.08 2.69 0.14 2.54 2.13 0.81 0.38 0.44 2.12 3.15 1.25\n",
            " 0.0 0.04 0.08 2.23 2.47 0.04 3.28 0.38 0.01 1.69 0.13 3.0 0.02 4.36 1.98\n",
            " 0.1 3.81 0.06 2.49 0.05 1.58 3.14 0.13 0.0 0.13 0.66 2.73 3.63 2.69 0.0\n",
            " 0.24 0.98 0.22 0.14 1.45 1.31 0.7 2.42 0.0 0.06 0.6 0.01 0.0 0.35 0.08\n",
            " 1.4 1.42 1.39 1.27 0.24 0.87 0.0 0.07 0.08 0.17 0.19 0.94 0.0 0.06 0.21\n",
            " 0.28 0.11 1.6 0.17 0.05 1.03 0.25 1.69 0.16 0.08 2.06 1.49 1.29 0.06 0.09\n",
            " 0.87 2.87 0.0 0.02 0.03 0.07 0.07 0.87 0.16 0.83 0.78 0.28 2.33 0.17 4.35\n",
            " 0.0 2.02 1.36 0.07 0.16 1.81 0.21 1.97 0.07 0.11 0.1 4.13 0.91 0.99 0.95\n",
            " 0.94 0.0 0.24 0.0 1.87 2.0 1.01 0.03 2.78 2.11 1.09 0.08 1.03 0.2 0.01\n",
            " 3.61 0.0 0.83 1.27 1.57 0.03 0.95 2.2 1.89 0.44 0.13 1.7 0.05 0.01 0.0\n",
            " 0.0 1.08 0.0 0.16 0.06 0.15 1.11 0.02 0.8 0.0 0.07 0.29 0.01 0.09 1.54\n",
            " 0.12 0.01 0.0 0.89 4.87 1.52 0.12 0.0 0.09 0.04 0.44 0.06 0.04 0.02 0.0\n",
            " 0.02 1.32 0.08 0.06 0.13 0.0 1.15 0.89 0.22 1.1 1.44 0.07 0.01 0.01 0.0\n",
            " 0.01 0.03 4.1 0.08 0.19 0.02 0.46 0.01 0.01 1.05 0.13 1.61 0.08 0.14 0.26\n",
            " 1.38 0.01 0.0 0.0 0.11 0.03 0.72 0.07 0.1 0.05 0.83 0.0 0.0 0.62 0.04 0.2\n",
            " 0.18 0.57 0.04 0.58 0.02 0.31 1.57 0.0 0.01 0.11 0.09 1.76 0.03 0.37 2.1\n",
            " 0.05 0.01 0.0 0.04 0.04 0.03 0.9 0.16 0.51 0.1 0.64 0.12 2.46 0.07 0.92\n",
            " 0.05 0.0 0.06 0.08 1.07 0.01 0.13 2.62 0.06 0.0 0.31 3.77 0.1 0.03 1.12\n",
            " 1.05 0.22 0.54 0.81 0.41 0.73 0.1 0.27 0.0 0.47 0.1 0.0 0.0 0.59 0.14\n",
            " 0.21 1.54 0.0 0.04 1.38 0.27 0.08 3.67 0.01 0.0 0.55 0.06 0.0 0.4 0.15\n",
            " 0.66 0.05 0.64 0.05 0.46 0.15 0.03 0.0 0.0 0.1 0.46 0.03 0.06 0.0 0.07\n",
            " 0.05 0.12 0.1 0.01 0.02 0.02 0.0 0.01 0.05 0.27 0.12 1.75 0.03 1.42 0.02\n",
            " 0.73 0.04 0.02 0.04 0.01 0.0 0.1 3.44 0.03 0.33 0.17 0.06 0.0 2.55 0.02\n",
            " 0.59 0.05 0.19 0.0 0.48 0.74 0.0 0.73 0.82 0.0 0.01 2.32 0.12 0.57 0.76\n",
            " 0.03 0.05 0.05 0.06 0.0 0.02 0.46 0.31 2.78 0.77 0.0 0.12 0.0 3.18 0.0\n",
            " 2.35 0.12 0.07 0.8 0.04 3.19 0.01 0.09 0.93 0.1 0.0]\n",
            "[[29.08 3.58 6.81 0.77]\n",
            " [15.85 12.88 3.79 3.31]\n",
            " [15.75 11.01 3.28 2.96]\n",
            " ...\n",
            " [1.18 0.87 0.93 0.2]\n",
            " [1.52 1.08 0.1 0.47]\n",
            " [1.86 1.02 0.0 0.29]]\n"
          ]
        }
      ],
      "source": [
        "# Cargar datos\n",
        "data = pd.read_csv((\"/content/drive/MyDrive/dataset/vgsales.csv\"),delimiter=',', skiprows=1)\n",
        "data= np.array(data)\n",
        "X = np.column_stack((data[:431,2:2],data[:431,4:4],data[:431,6:10]))\n",
        "\n",
        "\n",
        "y = data[:431, 8]\n",
        "m = y.size\n",
        "\n",
        "print(y)\n",
        "print(X)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def CambioDatos(x,y):\n",
        " \n",
        "\n",
        "  for i in range(m):\n",
        "    for j in range(4):\n",
        "      if x[i,j]== \"DS\":\n",
        "        x[i,j]= 1\n",
        "      if x[i,j]== \"PS2\":\n",
        "        x[i,j]= 2\n",
        "      if x[i,j]== \"PS3\":\n",
        "        x[i,j]= 3\n",
        "      if x[i,j]== \"Wii\":\n",
        "        x[i,j]= 4\n",
        "      if x[i,j]== \"X360\":\n",
        "        x[i,j]= 5\n",
        "      if x[i,j]== \"NES\":\n",
        "        x[i,j]= 6\n",
        "      if x[i,j]== \"GB\":\n",
        "        x[i,j]= 7\n",
        "      if x[i,j]== \"SNES\":\n",
        "        x[i,j]= 8\n",
        "      if x[i,j]== \"PS4\":\n",
        "        x[i,j]= 6\n",
        "      if x[i,j]== \".3DS\":\n",
        "        x[i,j]= 7\n",
        "      if x[i,j]== \"P.D\":\n",
        "        x[i,j]= 8\n",
        "      if x[i,j]== \"N64\":\n",
        "        x[i,j]= 8\n",
        "      if x[i,j]== \"PC\":\n",
        "        x[i,j]= 9\n",
        "      if x[i,j]== \"PSP\":\n",
        "        x[i,j]= 10\n",
        "      if x[i,j]== \"3DS\":\n",
        "        x[i,j]= 11\n",
        "      if x[i,j]== \"GBA\":\n",
        "        x[i,j]= 12\n",
        "      if x[i,j]== \"GC\":\n",
        "        x[i,j]= 13\n",
        "      if x[i,j]== \"PS\":\n",
        "        x[i,j]= 14\n",
        "      if x[i,j]== \"SAT\":\n",
        "        x[i,j]= 15\n",
        "      if x[i,j]== \"XB\":\n",
        "        x[i,j]= 16\n",
        "      if x[i,j]== \"PSV\":\n",
        "        x[i,j]= 17\n",
        "      if x[i,j]== \"XOne\":\n",
        "        x[i,j]= 18\n",
        "      if x[i,j]== \"DC\":\n",
        "        x[i,j]= 19\n",
        "      if x[i,j]== \"WiiU\":\n",
        "        x[i,j]= 20\n",
        "      if x[i,j]== \"GEN\":\n",
        "        x[i,j]= 21\n",
        "      if x[i,j]== \"SDC\":\n",
        "        x[i,j]= 22\n",
        "      if x[i,j]== \"Action\":\n",
        "        x[i,j]= 1\n",
        "      if x[i,j]== \"Sports\":\n",
        "        x[i,j]= 2\n",
        "      if x[i,j]== \"Racing\":\n",
        "        x[i,j]= 3\n",
        "      if x[i,j]== \"Role-Playing\":\n",
        "        x[i,j]= 4\n",
        "      if x[i,j]== \"Shooter\":\n",
        "        x[i,j]= 5\n",
        "      if x[i,j]== \"Platform\":\n",
        "        x[i,j]= 6\n",
        "      if x[i,j]== \"Puzzle\":\n",
        "        x[i,j]= 7\n",
        "      if x[i,j]== \"Fighting\":\n",
        "        x[i,j]= 8\n",
        "      if x[i,j]== \"Simulation\":\n",
        "        x[i,j]= 9\n",
        "      if x[i,j]== \"Adventure\":\n",
        "        x[i,j]= 10\n",
        "      if x[i,j]== \"Misc\":\n",
        "        x[i,j]= 11\n",
        "      if x[i,j]== \"Strategy\":\n",
        "        x[i,j]= 12\n",
        "      if x[i,j]== \"N/A\":\n",
        "        x[i,j]= 0\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "CambioDatos(X,y)\n",
        "X=np.array(X).astype(float)"
      ],
      "metadata": {
        "id": "ru_L23qz4vWy"
      },
      "execution_count": 479,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mGD_G8KqNhTv"
      },
      "execution_count": 479,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aidItZrFixeB"
      },
      "source": [
        "La desviación estándar es una forma de medir cuánta variación hay en el rango de valores de una característica en particular (la mayoría de los puntos caeran en un rango de ± 2 en relación a la desviaciones estándar de la media); esta es una alternativa a tomar el rango de valores (max-min). En `numpy`, se puede usar la función `std` para calcular la desviacion estandar. \n",
        "\n",
        "Por ejemplo, la caracteristica`X[:, 0]` contiene todos los valores de $x_1$ (tamaño de las casas) en el conjunto de entrenamiento, entonces `np.std(X[:, 0])` calcula la desviacion estandar de los tamaños de las casas.\n",
        "En el momento en que se llama a la función `featureNormalize`, la columna adicional de unos correspondiente a $ x_0 = 1 $ aún no se ha agregado a $ X $. \n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "**Notas para la implementación:** Cuando se normalize una caracteristica, es importante almacenar los valores usados para la normalización - el valor de la media y el valor de la desviación estandar usado para los calculos. Despues de aprender los parametros del modelo, se deseara predecir los precios de casas que no se han visto antes. Dado un nuevo valor de x (area del living room y el numero de dormitorios), primero se debe normalizar x usando la media y la desviacion estandar que se empleo anteriormente en el conjunto de entrenamiento para entrenar el modelo.\n",
        "</div>\n",
        "<a id=\"featureNormalize\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 480,
      "metadata": {
        "id": "ItYnCBRtixeC"
      },
      "outputs": [],
      "source": [
        "def  featureNormalize(X):\n",
        "    X_norm = X.copy()\n",
        "    mu = np.zeros(X.shape[1])\n",
        "    sigma = np.zeros(X.shape[1])\n",
        "\n",
        "    mu = np.mean(X, axis = 0)\n",
        "    sigma = np.std(X, axis = 0)\n",
        "    X_norm = (X - mu) / sigma\n",
        "    \n",
        "    return X_norm, mu, sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 481,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98uxsj3cixeC",
        "outputId": "45409b15-92f4-40e0-d6be-2b45a993ab45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[29.08  3.58  6.81  0.77]\n",
            "[15.85 12.88  3.79  3.31]\n",
            "[15.75 11.01  3.28  2.96]\n",
            "[11.27  8.89 10.22  1.  ]\n",
            "[23.2   2.26  4.22  0.58]\n",
            "[11.38  9.23  6.5   2.9 ]\n",
            "[14.03  9.2   2.93  2.85]\n",
            "[14.59  7.06  4.7   2.26]\n",
            "[26.93  0.63  0.28  0.47]\n",
            "[ 9.07 11.    1.93  2.75]\n",
            "[9.81 7.57 4.13 1.92]\n",
            "[9.   6.18 7.2  0.71]\n",
            "[8.94 8.03 3.6  2.15]\n",
            "[9.09 8.59 2.53 1.79]\n",
            "[14.97  4.94  0.24  1.67]\n",
            "[7.01 9.27 0.97 4.14]\n",
            "[ 9.43  0.4   0.41 10.57]\n",
            "[12.78  3.75  3.54  0.55]\n",
            "[4.75 9.26 4.16 2.05]\n",
            "[6.42 4.52 6.04 1.37]\n",
            "[10.83  2.71  4.18  0.42]\n",
            "[9.54 3.44 3.84 0.46]\n",
            "[9.63 5.31 0.06 1.38]\n",
            "[8.41 5.49 0.47 1.78]\n",
            "[6.06 3.9  5.38 0.5 ]\n",
            "[5.57 3.28 5.65 0.82]\n",
            "[3.44 5.36 5.32 1.18]\n",
            "[6.85 5.09 1.87 1.16]\n",
            "[9.03 4.28 0.13 1.32]\n",
            "[5.89 5.04 3.12 0.59]\n",
            "[9.67 3.73 0.11 1.13]\n",
            "[5.17 4.05 4.34 0.79]\n",
            "[5.77 5.81 0.35 2.31]\n",
            "[4.99 5.88 0.65 2.52]\n",
            "[8.25 4.3  0.07 1.12]\n",
            "[8.52 3.63 0.08 1.29]\n",
            "[5.54 5.82 0.49 1.62]\n",
            "[6.99 4.51 0.3  1.3 ]\n",
            "[6.75 2.61 2.66 1.02]\n",
            "[5.98 4.44 0.48 1.83]\n",
            "[2.55 3.52 5.33 0.88]\n",
            "[4.74 3.91 2.67 0.89]\n",
            "[7.97 2.83 0.13 1.21]\n",
            "[3.8  5.81 0.36 2.02]\n",
            "[4.4  2.77 3.96 0.77]\n",
            "[6.91 2.85 1.91 0.23]\n",
            "[3.01 0.01 1.1  7.53]\n",
            "[6.16 3.4  1.2  0.76]\n",
            "[4.23 3.37 3.08 0.65]\n",
            "[6.16 2.04 2.69 0.29]\n",
            "[6.76 3.1  0.14 1.03]\n",
            "[4.02 3.87 2.54 0.52]\n",
            "[4.89 2.99 2.13 0.78]\n",
            "[2.96 4.88 0.81 2.12]\n",
            "[4.99 3.69 0.38 1.63]\n",
            "[4.76 3.76 0.44 1.62]\n",
            "[5.99 2.15 2.12 0.29]\n",
            "[4.34 2.65 3.15 0.35]\n",
            "[5.08 3.11 1.25 0.98]\n",
            "[6.05 3.15 0.   1.07]\n",
            "[6.72 2.63 0.04 0.82]\n",
            "[7.03 1.98 0.08 0.78]\n",
            "[5.55 1.94 2.23 0.15]\n",
            "[3.66 3.07 2.47 0.63]\n",
            "[6.63 2.36 0.04 0.73]\n",
            "[3.01 2.47 3.28 0.96]\n",
            "[4.09 3.73 0.38 1.38]\n",
            "[5.84 2.89 0.01 0.78]\n",
            "[3.88 3.42 1.69 0.5 ]\n",
            "[5.91 2.38 0.13 0.9 ]\n",
            "[4.36 1.71 3.   0.23]\n",
            "[5.58 2.83 0.02 0.77]\n",
            "[2.01 2.32 4.36 0.41]\n",
            "[4.46 1.88 1.98 0.7 ]\n",
            "[5.03 2.86 0.1  0.85]\n",
            "[3.54 1.24 3.81 0.18]\n",
            "[1.11 6.06 0.06 1.26]\n",
            "[1.79 3.53 2.49 0.68]\n",
            "[6.82 1.53 0.05 0.08]\n",
            "[3.81 2.3  1.58 0.73]\n",
            "[2.91 1.86 3.14 0.43]\n",
            "[1.06 5.05 0.13 2.01]\n",
            "[0.98 6.42 0.   0.71]\n",
            "[5.8  2.01 0.13 0.15]\n",
            "[2.58 3.9  0.66 0.91]\n",
            "[2.91 2.07 2.73 0.33]\n",
            "[2.28 1.72 3.63 0.23]\n",
            "[2.82 1.78 2.69 0.55]\n",
            "[7.28 0.45 0.   0.08]\n",
            "[2.9  2.83 0.24 1.75]\n",
            "[3.66 2.42 0.98 0.64]\n",
            "[2.93 3.29 0.22 1.23]\n",
            "[2.8  3.3  0.14 1.37]\n",
            "[4.1  1.89 1.45 0.16]\n",
            "[3.78 2.17 1.31 0.31]\n",
            "[5.39 1.18 0.7  0.19]\n",
            "[3.24 1.35 2.42 0.43]\n",
            "[4.79 1.9  0.   0.69]\n",
            "[4.46 2.13 0.06 0.69]\n",
            "[3.83 2.19 0.6  0.7 ]\n",
            "[4.52 2.09 0.01 0.67]\n",
            "[3.51 3.03 0.   0.73]\n",
            "[2.85 2.93 0.35 1.1 ]\n",
            "[3.27 2.83 0.08 1.02]\n",
            "[3.27 2.22 1.4  0.29]\n",
            "[3.68 1.75 1.42 0.28]\n",
            "[4.41 1.04 1.39 0.22]\n",
            "[3.13 2.07 1.27 0.49]\n",
            "[2.47 3.15 0.24 1.1 ]\n",
            "[4.12 1.77 0.87 0.19]\n",
            "[4.14 2.21 0.   0.56]\n",
            "[0.78 4.32 0.07 1.73]\n",
            "[2.71 3.02 0.08 1.09]\n",
            "[2.93 2.75 0.17 0.99]\n",
            "[2.77 2.8  0.19 1.06]\n",
            "[3.23 2.35 0.94 0.3 ]\n",
            "[3.5  2.64 0.   0.67]\n",
            "[4.15 1.92 0.06 0.64]\n",
            "[3.27 2.25 0.21 1.  ]\n",
            "[3.1  2.3  0.28 1.04]\n",
            "[0.84 4.32 0.11 1.42]\n",
            "[1.67 2.78 1.6  0.62]\n",
            "[2.79 2.61 0.17 1.03]\n",
            "[0.79 4.29 0.05 1.47]\n",
            "[3.25 1.84 1.03 0.47]\n",
            "[2.55 2.71 0.25 1.05]\n",
            "[3.74 0.93 1.69 0.14]\n",
            "[2.64 2.56 0.16 1.14]\n",
            "[4.98 1.3  0.08 0.07]\n",
            "[2.57 1.58 2.06 0.21]\n",
            "[3.64 1.2  1.49 0.07]\n",
            "[2.55 1.56 1.29 0.99]\n",
            "[4.34 1.35 0.06 0.61]\n",
            "[3.7  1.97 0.09 0.57]\n",
            "[4.01 1.26 0.87 0.17]\n",
            "[2.47 0.83 2.87 0.12]\n",
            "[0.07 6.21 0.   0.  ]\n",
            "[3.11 2.8  0.02 0.33]\n",
            "[3.92 1.78 0.03 0.51]\n",
            "[4.05 1.62 0.07 0.49]\n",
            "[3.54 1.9  0.07 0.6 ]\n",
            "[2.45 2.01 0.87 0.72]\n",
            "[4.47 1.2  0.16 0.19]\n",
            "[2.63 1.74 0.83 0.83]\n",
            "[3.18 1.83 0.78 0.24]\n",
            "[2.41 2.28 0.28 1.01]\n",
            "[1.88 0.   2.33 1.74]\n",
            "[2.8  2.05 0.17 0.9 ]\n",
            "[0.66 0.69 4.35 0.15]\n",
            "[3.66 1.63 0.   0.53]\n",
            "[1.88 1.47 2.02 0.45]\n",
            "[2.26 1.89 1.36 0.23]\n",
            "[3.13 1.94 0.07 0.58]\n",
            "[2.49 2.05 0.16 0.96]\n",
            "[2.97 0.69 1.81 0.11]\n",
            "[2.54 1.95 0.21 0.87]\n",
            "[2.95 0.6  1.97 0.04]\n",
            "[3.28 1.65 0.07 0.55]\n",
            "[2.7  1.91 0.11 0.8 ]\n",
            "[2.99 1.92 0.1  0.51]\n",
            "[0.47 0.57 4.13 0.34]\n",
            "[3.14 1.24 0.91 0.2 ]\n",
            "[2.62 1.64 0.99 0.23]\n",
            "[3.21 1.11 0.95 0.2 ]\n",
            "[3.18 1.24 0.94 0.09]\n",
            "[2.72 1.87 0.   0.84]\n",
            "[2.07 2.29 0.24 0.82]\n",
            "[1.97 2.51 0.   0.94]\n",
            "[1.74 1.24 1.87 0.52]\n",
            "[2.18 0.96 2.   0.2 ]\n",
            "[3.02 1.12 1.01 0.16]\n",
            "[3.13 1.71 0.03 0.44]\n",
            "[1.62 0.77 2.78 0.14]\n",
            "[1.92 1.08 2.11 0.17]\n",
            "[3.33 0.79 1.09 0.06]\n",
            "[3.1  1.56 0.08 0.51]\n",
            "[1.22 2.48 1.03 0.52]\n",
            "[2.3  2.46 0.2  0.28]\n",
            "[4.26 0.26 0.01 0.71]\n",
            "[0.65 0.75 3.61 0.2 ]\n",
            "[2.43 2.15 0.   0.62]\n",
            "[2.93 1.25 0.83 0.2 ]\n",
            "[2.32 1.3  1.27 0.31]\n",
            "[2.49 0.98 1.57 0.15]\n",
            "[1.08 3.48 0.03 0.58]\n",
            "[1.9  1.83 0.95 0.49]\n",
            "[2.1  0.74 2.2  0.11]\n",
            "[0.96 2.02 1.89 0.28]\n",
            "[1.64 2.48 0.44 0.58]\n",
            "[1.98 2.23 0.13 0.8 ]\n",
            "[2.71 0.61 1.7  0.11]\n",
            "[3.59 1.11 0.05 0.38]\n",
            "[3.21 1.53 0.01 0.38]\n",
            "[3.22 1.69 0.   0.2 ]\n",
            "[3.81 0.63 0.   0.68]\n",
            "[1.96 1.43 1.08 0.65]\n",
            "[2.66 2.01 0.   0.41]\n",
            "[1.7  2.02 0.16 1.21]\n",
            "[0.6  3.29 0.06 1.13]\n",
            "[3.4  1.3  0.15 0.22]\n",
            "[2.05 1.16 1.11 0.73]\n",
            "[3.42 1.38 0.02 0.2 ]\n",
            "[2.59 1.06 0.8  0.57]\n",
            "[2.79 1.89 0.   0.33]\n",
            "[3.36 1.36 0.07 0.21]\n",
            "[3.06 1.18 0.29 0.46]\n",
            "[3.49 0.01 0.01 1.48]\n",
            "[3.39 1.03 0.09 0.44]\n",
            "[1.85 1.2  1.54 0.37]\n",
            "[2.31 1.73 0.12 0.78]\n",
            "[3.98 0.26 0.01 0.66]\n",
            "[2.89 1.54 0.   0.46]\n",
            "[2.91 0.99 0.89 0.1 ]\n",
            "[0.   0.   4.87 0.  ]\n",
            "[2.62 0.6  1.52 0.1 ]\n",
            "[2.74 1.36 0.12 0.63]\n",
            "[2.56 1.68 0.   0.59]\n",
            "[1.91 2.   0.09 0.83]\n",
            "[0.57 3.14 0.04 1.07]\n",
            "[2.57 1.57 0.44 0.21]\n",
            "[0.28 3.75 0.06 0.69]\n",
            "[2.99 1.31 0.04 0.41]\n",
            "[2.36 2.1  0.02 0.25]\n",
            "[1.73 2.19 0.   0.79]\n",
            "[3.05 1.41 0.02 0.2 ]\n",
            "[1.87 1.12 1.32 0.37]\n",
            "[1.94 1.95 0.08 0.7 ]\n",
            "[2.08 2.04 0.06 0.47]\n",
            "[2.29 1.97 0.13 0.24]\n",
            "[3.06 1.12 0.   0.44]\n",
            "[2.42 0.91 1.15 0.13]\n",
            "[2.6  0.99 0.89 0.13]\n",
            "[1.89 1.99 0.22 0.48]\n",
            "[1.78 1.39 1.1  0.3 ]\n",
            "[1.55 1.15 1.44 0.43]\n",
            "[1.78 1.87 0.07 0.82]\n",
            "[3.19 0.92 0.01 0.42]\n",
            "[4.18 0.26 0.01 0.08]\n",
            "[4.21 0.24 0.   0.05]\n",
            "[3.63 0.24 0.01 0.61]\n",
            "[2.71 1.51 0.03 0.23]\n",
            "[0.2  0.14 4.1  0.02]\n",
            "[1.96 1.69 0.08 0.74]\n",
            "[1.54 1.94 0.19 0.77]\n",
            "[2.71 1.29 0.02 0.43]\n",
            "[2.55 1.11 0.46 0.33]\n",
            "[2.67 1.35 0.01 0.39]\n",
            "[2.66 1.29 0.01 0.46]\n",
            "[0.1  2.39 1.05 0.86]\n",
            "[2.82 1.05 0.13 0.4 ]\n",
            "[2.19 0.5  1.61 0.08]\n",
            "[2.03 1.79 0.08 0.47]\n",
            "[1.73 1.73 0.14 0.75]\n",
            "[3.03 0.91 0.26 0.13]\n",
            "[2.2  0.58 1.38 0.17]\n",
            "[0.92 2.93 0.01 0.46]\n",
            "[2.75 1.18 0.   0.37]\n",
            "[4.   0.26 0.   0.05]\n",
            "[2.51 1.27 0.11 0.41]\n",
            "[2.64 1.2  0.03 0.39]\n",
            "[2.11 1.11 0.72 0.3 ]\n",
            "[2.23 1.34 0.07 0.61]\n",
            "[1.41 2.02 0.1  0.72]\n",
            "[3.   1.11 0.05 0.07]\n",
            "[1.46 0.   0.83 1.93]\n",
            "[2.45 1.02 0.   0.75]\n",
            "[1.7  2.27 0.   0.23]\n",
            "[2.03 1.27 0.62 0.3 ]\n",
            "[0.78 2.55 0.04 0.84]\n",
            "[0.88 2.3  0.2  0.83]\n",
            "[1.3  2.07 0.18 0.65]\n",
            "[1.28 1.83 0.57 0.53]\n",
            "[2.25 1.47 0.04 0.43]\n",
            "[2.02 1.06 0.58 0.53]\n",
            "[0.84 2.79 0.02 0.53]\n",
            "[3.38 0.44 0.31 0.04]\n",
            "[2.04 0.48 1.57 0.07]\n",
            "[3.79 0.27 0.   0.11]\n",
            "[3.36 0.21 0.01 0.56]\n",
            "[1.4  1.86 0.11 0.77]\n",
            "[4.03 0.   0.09 0.  ]\n",
            "[1.65 0.61 1.76 0.09]\n",
            "[0.71 2.48 0.03 0.89]\n",
            "[2.14 1.2  0.37 0.4 ]\n",
            "[1.42 0.51 2.1  0.07]\n",
            "[2.13 1.5  0.05 0.42]\n",
            "[2.45 1.26 0.01 0.37]\n",
            "[2.57 1.52 0.   0.  ]\n",
            "[2.65 1.06 0.04 0.33]\n",
            "[2.32 0.04 0.04 1.67]\n",
            "[2.35 1.28 0.03 0.41]\n",
            "[0.12 2.26 0.9  0.77]\n",
            "[2.28 1.55 0.16 0.06]\n",
            "[1.68 1.51 0.51 0.35]\n",
            "[1.12 2.12 0.1  0.69]\n",
            "[2.78 0.58 0.64 0.04]\n",
            "[1.38 1.87 0.12 0.65]\n",
            "[1.22 0.28 2.46 0.04]\n",
            "[2.15 1.2  0.07 0.59]\n",
            "[0.92 1.78 0.92 0.37]\n",
            "[2.67 0.89 0.05 0.37]\n",
            "[2.08 1.35 0.   0.54]\n",
            "[2.1  1.36 0.06 0.4 ]\n",
            "[1.18 1.96 0.08 0.7 ]\n",
            "[1.97 0.76 1.07 0.11]\n",
            "[2.29 1.17 0.01 0.42]\n",
            "[1.33 1.71 0.13 0.73]\n",
            "[0.67 0.49 2.62 0.11]\n",
            "[1.53 1.61 0.06 0.67]\n",
            "[1.15 2.09 0.   0.64]\n",
            "[0.93 1.94 0.31 0.7 ]\n",
            "[0.1  0.   3.77 0.  ]\n",
            "[2.12 1.14 0.1  0.51]\n",
            "[2.48 0.65 0.03 0.69]\n",
            "[0.16 1.89 1.12 0.68]\n",
            "[0.87 1.57 1.05 0.35]\n",
            "[2.12 1.44 0.22 0.06]\n",
            "[2.21 0.96 0.54 0.13]\n",
            "[2.26 0.48 0.81 0.27]\n",
            "[1.06 1.93 0.41 0.43]\n",
            "[1.44 1.37 0.73 0.27]\n",
            "[1.49 1.58 0.1  0.61]\n",
            "[1.14 1.91 0.27 0.46]\n",
            "[2.4  1.03 0.   0.36]\n",
            "[1.82 1.24 0.47 0.25]\n",
            "[1.4  1.4  0.1  0.87]\n",
            "[1.98 1.47 0.   0.32]\n",
            "[2.03 1.56 0.   0.17]\n",
            "[1.98 0.88 0.59 0.32]\n",
            "[1.37 2.   0.14 0.22]\n",
            "[0.96 2.   0.21 0.56]\n",
            "[1.3  0.77 1.54 0.11]\n",
            "[1.93 1.58 0.   0.19]\n",
            "[0.58 2.48 0.04 0.59]\n",
            "[1.49 0.73 1.38 0.1 ]\n",
            "[1.3  1.51 0.27 0.61]\n",
            "[1.59 1.61 0.08 0.41]\n",
            "[0.   0.   3.67 0.  ]\n",
            "[2.13 1.18 0.01 0.35]\n",
            "[1.65 1.22 0.   0.79]\n",
            "[1.87 1.13 0.55 0.1 ]\n",
            "[2.53 0.81 0.06 0.24]\n",
            "[2.33 0.97 0.   0.35]\n",
            "[0.71 1.8  0.4  0.74]\n",
            "[2.08 1.09 0.15 0.33]\n",
            "[2.23 0.68 0.66 0.06]\n",
            "[0.6  2.46 0.05 0.52]\n",
            "[0.05 0.   0.64 2.93]\n",
            "[1.78 1.42 0.05 0.38]\n",
            "[2.08 0.83 0.46 0.25]\n",
            "[1.28 1.61 0.15 0.57]\n",
            "[1.93 1.22 0.03 0.44]\n",
            "[2.05 1.4  0.   0.16]\n",
            "[2.02 1.17 0.   0.42]\n",
            "[1.61 1.5  0.1  0.39]\n",
            "[2.38 0.67 0.46 0.1 ]\n",
            "[2.18 1.02 0.03 0.37]\n",
            "[2.01 1.35 0.06 0.16]\n",
            "[1.57 1.79 0.   0.2 ]\n",
            "[1.56 1.4  0.07 0.5 ]\n",
            "[1.23 1.77 0.05 0.49]\n",
            "[1.66 1.58 0.12 0.18]\n",
            "[1.9  1.13 0.1  0.41]\n",
            "[1.98 1.14 0.01 0.41]\n",
            "[2.14 1.08 0.02 0.29]\n",
            "[0.71 2.4  0.02 0.4 ]\n",
            "[1.96 1.33 0.   0.23]\n",
            "[2.14 1.21 0.01 0.17]\n",
            "[2.66 0.5  0.05 0.3 ]\n",
            "[1.22 1.66 0.27 0.38]\n",
            "[2.11 0.94 0.12 0.34]\n",
            "[1.17 0.5  1.75 0.08]\n",
            "[2.84 0.39 0.03 0.24]\n",
            "[1.64 0.38 1.42 0.06]\n",
            "[2.2  0.97 0.02 0.31]\n",
            "[0.59 1.83 0.73 0.35]\n",
            "[0.59 2.36 0.04 0.51]\n",
            "[1.94 1.22 0.02 0.31]\n",
            "[2.09 1.02 0.04 0.32]\n",
            "[2.11 1.01 0.01 0.35]\n",
            "[2.26 0.89 0.   0.3 ]\n",
            "[0.88 1.75 0.1  0.72]\n",
            "[0.   0.   3.44 0.  ]\n",
            "[2.39 0.73 0.03 0.29]\n",
            "[1.55 1.27 0.33 0.29]\n",
            "[1.34 1.54 0.17 0.38]\n",
            "[1.82 1.07 0.06 0.47]\n",
            "[1.13 2.07 0.   0.22]\n",
            "[0.86 0.   2.55 0.02]\n",
            "[1.75 1.2  0.02 0.43]\n",
            "[1.73 0.69 0.59 0.4 ]\n",
            "[0.46 2.28 0.05 0.61]\n",
            "[1.56 1.47 0.19 0.17]\n",
            "[2.03 1.03 0.   0.32]\n",
            "[1.08 1.35 0.48 0.47]\n",
            "[1.43 0.94 0.74 0.27]\n",
            "[2.47 0.76 0.   0.13]\n",
            "[1.9  0.67 0.73 0.06]\n",
            "[0.65 1.61 0.82 0.28]\n",
            "[1.63 1.53 0.   0.18]\n",
            "[1.9  1.14 0.01 0.29]\n",
            "[0.   0.99 2.32 0.02]\n",
            "[1.45 1.29 0.12 0.46]\n",
            "[1.44 1.01 0.57 0.3 ]\n",
            "[1.15 1.17 0.76 0.24]\n",
            "[1.47 1.39 0.03 0.43]\n",
            "[1.99 1.05 0.05 0.22]\n",
            "[1.5  1.28 0.05 0.46]\n",
            "[0.8  1.92 0.06 0.5 ]\n",
            "[2.13 0.92 0.   0.23]\n",
            "[1.89 1.05 0.02 0.31]\n",
            "[1.36 1.13 0.46 0.32]\n",
            "[0.5  1.59 0.31 0.87]\n",
            "[0.25 0.19 2.78 0.04]\n",
            "[0.95 1.3  0.77 0.22]\n",
            "[0.88 2.11 0.   0.23]\n",
            "[1.27 1.33 0.12 0.51]\n",
            "[2.33 0.3  0.   0.59]\n",
            "[0.03 0.   3.18 0.  ]\n",
            "[1.72 1.33 0.   0.16]\n",
            "[0.73 0.1  2.35 0.02]\n",
            "[2.26 0.72 0.12 0.1 ]\n",
            "[1.76 1.21 0.07 0.16]\n",
            "[1.35 0.6  0.8  0.44]\n",
            "[1.48 1.01 0.04 0.66]\n",
            "[0.   0.   3.19 0.  ]\n",
            "[2.15 0.77 0.01 0.26]\n",
            "[1.78 1.12 0.09 0.19]\n",
            "[1.18 0.87 0.93 0.2 ]\n",
            "[1.52 1.08 0.1  0.47]\n",
            "[1.86 1.02 0.   0.29]\n",
            "[[29.08  3.58  6.81  0.77]\n",
            " [15.85 12.88  3.79  3.31]\n",
            " [15.75 11.01  3.28  2.96]\n",
            " ...\n",
            " [ 1.18  0.87  0.93  0.2 ]\n",
            " [ 1.52  1.08  0.1   0.47]\n",
            " [ 1.86  1.02  0.    0.29]]\n",
            "Media calculada: [3.16208817 2.01772622 0.89064965 0.62990719]\n",
            "Desviación estandar calculada: [3.10581848 1.73308636 1.38327683 0.8040492 ]\n",
            "[[ 8.34495385  0.90144024  4.27922322  0.17423412]\n",
            " [ 4.08520715  6.26758945  2.09600152  3.33324478]\n",
            " [ 4.05300952  5.18858955  1.72731176  2.89794804]\n",
            " ...\n",
            " [-0.63818545 -0.6622441   0.0284472  -0.53467772]\n",
            " [-0.5287135  -0.54107299 -0.57157731 -0.19887737]\n",
            " [-0.41924155 -0.57569331 -0.64386942 -0.42274427]]\n"
          ]
        }
      ],
      "source": [
        "# llama featureNormalize con los datos cargados\n",
        "for i in range (m):\n",
        "  print(X[i])\n",
        "X_norm, mu, sigma = featureNormalize(X)\n",
        "\n",
        "print(X)\n",
        "print('Media calculada:', mu)\n",
        "print('Desviación estandar calculada:', sigma)\n",
        "print(X_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60-ukRpMixeD"
      },
      "source": [
        "Despues de `featureNormalize` la funcion es provada, se añade el temino de interseccion a `X_norm`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 482,
      "metadata": {
        "id": "6d3wMkViixeD"
      },
      "outputs": [],
      "source": [
        "# Añade el termino de interseccion a X\n",
        "# (Columna de unos para X0)\n",
        "X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 483,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUMhtErHixeD",
        "outputId": "ea843005-bad6-49e9-945a-e9ac77e1ff46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.          8.34495385  0.90144024  4.27922322  0.17423412]\n",
            " [ 1.          4.08520715  6.26758945  2.09600152  3.33324478]\n",
            " [ 1.          4.05300952  5.18858955  1.72731176  2.89794804]\n",
            " ...\n",
            " [ 1.         -0.63818545 -0.6622441   0.0284472  -0.53467772]\n",
            " [ 1.         -0.5287135  -0.54107299 -0.57157731 -0.19887737]\n",
            " [ 1.         -0.41924155 -0.57569331 -0.64386942 -0.42274427]]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYKmdg1vixeE"
      },
      "source": [
        "<a id=\"section5\"></a>\n",
        "### 2.2 Descenso por el gradiente\n",
        "\n",
        "En el ejemplo anterior se implemento el descenso por el gradiente para un problema de regresion univariable. La unica diferencia es que ahora existe una caracteristica adicional en la matriz $X$. La función de hipótesis y la regla de actualización del descenso del gradiente por lotes permanecen sin cambios.\n",
        "\n",
        "La implementacion de las funciones `computeCostMulti` y `gradientDescentMulti` son similares a la funcion de costo y función de descenso por el gradiente de la regresión lineal multiple es similar al de la regresion lineal multivariable. Es importante garantizar que el codigo soporte cualquier numero de caracteristicas y esten bien vectorizadas.\n",
        "\n",
        "Se puede utilizar `shape`, propiedad de los arrays `numpy`, para identificar cuantas caracteristicas estan consideradas en el dataset.\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "**Nota de implementación:** En el caso de multivariables, la función de costo puede se escrita considerando la forma vectorizada de la siguiente manera:\n",
        "\n",
        "$$ J(\\theta) = \\frac{1}{2m}(X\\theta - \\vec{y})^T(X\\theta - \\vec{y}) $$\n",
        "\n",
        "donde:\n",
        "\n",
        "$$ X = \\begin{pmatrix}\n",
        "          - (x^{(1)})^T - \\\\\n",
        "          - (x^{(2)})^T - \\\\\n",
        "          \\vdots \\\\\n",
        "          - (x^{(m)})^T - \\\\ \\\\\n",
        "        \\end{pmatrix} \\qquad \\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\\\\\end{bmatrix}$$\n",
        "\n",
        "La version vectorizada es eficiente cuando se trabaja con herramientas de calculo numericos computacional como `numpy`. \n",
        "</div>\n",
        "\n",
        "<a id=\"computeCostMulti\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 484,
      "metadata": {
        "id": "Z8Vfnu1KixeF"
      },
      "outputs": [],
      "source": [
        "def computeCostMulti(X, y, theta):\n",
        "    # Inicializa algunos valores utiles\n",
        "    m = y.shape[0] # numero de ejemplos de entrenamiento\n",
        "    \n",
        "    J = 0\n",
        "    \n",
        "    h = np.dot(X, theta)\n",
        "    \n",
        "    J = (1/(2 * m)) * np.sum(np.square(np.dot(X, theta) - y))\n",
        "    \n",
        "    return J\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 485,
      "metadata": {
        "id": "Z2pHrF-zixeF"
      },
      "outputs": [],
      "source": [
        "def gradientDescentMulti(X, y, theta, alpha, num_iters):\n",
        "    \n",
        "    # Inicializa algunos valores \n",
        "    m = y.shape[0] # numero de ejemplos de entrenamiento\n",
        "    \n",
        "    # realiza una copia de theta, el cual será acutalizada por el descenso por el gradiente\n",
        "    theta = theta.copy()\n",
        "    \n",
        "    J_history = []\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "        theta = theta - (alpha / m) * (np.dot(X, theta) - y).dot(X)\n",
        "        J_history.append(computeCostMulti(X, y, theta))\n",
        "    \n",
        "    return theta, J_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y6Mnr1OixeF"
      },
      "source": [
        "#### 3.2.1 Seleccionando coheficientes de aprendizaje\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 486,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "TtB6P_D8ixeG",
        "outputId": "7a141daf-7623-481e-e182-7746065948d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.          8.34495385  0.90144024  4.27922322  0.17423412]\n",
            " [ 1.          4.08520715  6.26758945  2.09600152  3.33324478]\n",
            " [ 1.          4.05300952  5.18858955  1.72731176  2.89794804]\n",
            " ...\n",
            " [ 1.         -0.63818545 -0.6622441   0.0284472  -0.53467772]\n",
            " [ 1.         -0.5287135  -0.54107299 -0.57157731 -0.19887737]\n",
            " [ 1.         -0.41924155 -0.57569331 -0.64386942 -0.42274427]]\n",
            "theta calculado por el descenso por el gradiente: [0.8906496519721551 -2.079869514280257e-08 7.163302479482604e-08\n",
            " 1.3832768104451119 -4.647456081561549e-08]\n",
            "La salud fetal (usando el descenso por el gradiente): 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Elegir algun valor para alpha (probar varias alternativas)\n",
        "print(X)\n",
        "alpha = 0.02\n",
        "num_iters = 1700\n",
        "\n",
        "\n",
        "# inicializa theta y ejecuta el descenso por el gradiente\n",
        "theta = np.zeros(5)\n",
        "theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)\n",
        "\n",
        "# Grafica la convergencia del costo\n",
        "pyplot.plot(np.arange(len(J_history)), J_history, lw=2)\n",
        "pyplot.xlabel('Numero de iteraciones')\n",
        "pyplot.ylabel('Costo J')\n",
        "\n",
        "# Muestra los resultados del descenso por el gradiente\n",
        "print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta)))\n",
        "\n",
        "# Estimar el precio para una casa de 1650 sq-ft, con 3 dormitorios\n",
        "X_array = [1, 10.58760203, 12.44014015,  2.06589141,  8.79869606]\n",
        "X_array[1:5] = (X_array[1:5] - mu) / sigma\n",
        "price = np.dot(X_array, theta)   # Se debe cambiar esto\n",
        "\n",
        "print('La salud fetal (usando el descenso por el gradiente): {:.0f}'.format(price))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 487,
      "metadata": {
        "id": "lPn4NezUixeG"
      },
      "outputs": [],
      "source": [
        "X_array = [1, 10.58760203, 12.44014015,  2.06589141,  8.79869606]\n",
        "X_array[1:5] = (X_array[1:5] - mu) / sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 488,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1zgpbf9ixeG",
        "outputId": "eb6cb44f-2545-43bf-cc69-8a9e8e418a46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.3908396200101434, 6.013787984646985, 0.8496070558563731, 10.159563430472605]"
            ]
          },
          "metadata": {},
          "execution_count": 488
        }
      ],
      "source": [
        "X_array[1:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBYG2VZGixeH"
      },
      "source": [
        "<a id=\"section7\"></a>\n",
        "### 2.3 Ecuacion de la Normal\n",
        "\n",
        "Una manera de calcular rapidamente el modelo de una regresion lineal es:\n",
        "\n",
        "$$ \\theta = \\left( X^T X\\right)^{-1} X^T\\vec{y}$$\n",
        "\n",
        "Utilizando esta formula no requiere que se escale ninguna caracteristica, y se obtendra una solucion exacta con un solo calculo: no hay “bucles de convergencia” como en el descenso por el gradiente. \n",
        "\n",
        "Primero se recargan los datos para garantizar que las variables no esten modificadas. Recordar que no es necesario escalar las caracteristicas, se debe agregar la columna de unos a la matriz $X$ para tener el termino de intersección($\\theta_0$). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 489,
      "metadata": {
        "id": "8Gn2aeGUixeH"
      },
      "outputs": [],
      "source": [
        "# Cargar datos\n",
        "data = pd.read_csv((\"/content/drive/MyDrive/dataset/vgsales.csv\"), delimiter=',', skiprows=1)\n",
        "data= np.array(data)\n",
        "X = np.column_stack((data[:431,2:2],data[:431,4:4],data[:431,6:10]))\n",
        "\n",
        "\n",
        "y = data[:431, 8]\n",
        "m = y.size\n",
        "X = np.concatenate([np.ones((m, 1)), X], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 490,
      "metadata": {
        "id": "Ggl3eon1ixeH"
      },
      "outputs": [],
      "source": [
        "def normalEqn(X, y):\n",
        "    X = float64\n",
        "  \n",
        "    theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),y)\n",
        "    \n",
        "    \n",
        "    return theta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula los parametros con la ecuación de la normal\n",
        "theta = normalEqn(X, y);\n",
        "# Muestra los resultados optenidos a partir de la aplicación de la ecuación de la normal\n",
        "print('Theta calculado a partir de la ecuación de la normal: {:s}'.format(str(theta)));\n",
        "\n",
        "# Estimar el precio para una casa de superficie de 1650 sq-ft y tres dormitorios\n",
        "\n",
        "X_array = [1, 10.58760203, 12.44014015,  2.06589141,  8.79869606]\n",
        "price = np.dot(X_array, theta) \n",
        "\n",
        "print('Precio predecido para una cada de superficie de 1650 sq-ft y 3 dormitorios (usando la ecuación de la normal): ${:.0f}'.format(price))"
      ],
      "metadata": {
        "id": "5Ip7frxJonzB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "89c1e5897a05c230b641cbe6795c273f19c63c261829cbeaac49beeeb28f3eb6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}